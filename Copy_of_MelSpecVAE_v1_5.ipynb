{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGsLH8ZeE-EK"
      },
      "source": [
        "# MelSpecVAE v.1.5\n",
        " Author: Moisés Horta Valenzuela, 2021\n",
        "> Website: [moiseshorta.audio](https://moiseshorta.audio)\n",
        "\n",
        "> Twitter: [@hexorcismos](https://twitter.com/hexorcismos)\n",
        "\n",
        "\n",
        "```\n",
        "MelSpecVAE is a Variational Autoencoder which synthesizes Mel-Spectrograms thay can be inverted into raw audio waveform.\n",
        "Currently you can train it with any dataset of .wav audio at 44.1khz Sample Rate and 16bit bitdepth.\n",
        "\n",
        "> Features:\n",
        "* Interpolate through 2 different points in the latent space and synthesize the 'in between' sounds.\n",
        "* Generate short one-shot audio\n",
        "* Synthesize arbitrarily long audio samples by generating seeds and sampling from the latent space.\n",
        "\n",
        "> Credits:\n",
        "* VAE neural network architecture coded following 'The Sound of AI' Youtube tutorial series by Valerio Velardo\n",
        "* Some utility functions from Marco Passini's MelGAN-VC Jupyter Notebook.\n",
        "\n",
        "> Last update:\n",
        "* 12.12.2021: Added experimental feature 'Timbre Transfer'\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbS8aCU-fMU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db63649-a3d9-44d3-dbe7-42c52c098339"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8p_G3UIaanv"
      },
      "source": [
        "## Run the next cells first for training, generating or timbre transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otzXYjIG_wmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4730bad4-a8a3-42ec-dcca-f5630267644e"
      },
      "source": [
        "#@title Import Tensorflow and torchaudio \n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0\n",
        "!pip install h5py==2.10.0 --force-reinstall\n",
        "!pip install soundfile                    #to save wav files\n",
        "!pip install --no-deps torchaudio==0.5\n",
        "!pip install git+https://github.com/pvigier/perlin-numpy #for generating perlin and fractal noise"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow-gpu==2.0.0\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 30.6 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Downloading numpy-1.24.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 58.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.0 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.24.0 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (0.11.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchaudio==0.5\n",
            "  Downloading torchaudio-0.5.0-cp38-cp38-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 34.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torchaudio\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.0+cu116\n",
            "    Uninstalling torchaudio-0.13.0+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.0+cu116\n",
            "Successfully installed torchaudio-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pvigier/perlin-numpy\n",
            "  Cloning https://github.com/pvigier/perlin-numpy to /tmp/pip-req-build-xq_r64d7\n",
            "  Running command git clone -q https://github.com/pvigier/perlin-numpy /tmp/pip-req-build-xq_r64d7\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from perlin-numpy==0.0.0) (1.24.0)\n",
            "Building wheels for collected packages: perlin-numpy\n",
            "  Building wheel for perlin-numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for perlin-numpy: filename=perlin_numpy-0.0.0-py3-none-any.whl size=4752 sha256=194f11748cbea05c05a17a4aa9b633189331eb0412412252a453537bddaedbc2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-11l2mqep/wheels/42/f1/aa/a01cebc65547819ce7e859ed78b46258e145fd7cdc29147efd\n",
            "Successfully built perlin-numpy\n",
            "Installing collected packages: perlin-numpy\n",
            "Successfully installed perlin-numpy-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/pvigier/perlin-numpy #for generating perlin and fractal noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJU1m3yXUxdp",
        "outputId": "65137e8f-9836-45c6-f343-12cdc8a4b5db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/pvigier/perlin-numpy\n",
            "  Cloning https://github.com/pvigier/perlin-numpy to /tmp/pip-req-build-_v1c60eq\n",
            "  Running command git clone -q https://github.com/pvigier/perlin-numpy /tmp/pip-req-build-_v1c60eq\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from perlin-numpy==0.0.0) (1.21.6)\n",
            "Building wheels for collected packages: perlin-numpy\n",
            "  Building wheel for perlin-numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for perlin-numpy: filename=perlin_numpy-0.0.0-py3-none-any.whl size=4752 sha256=5af5396cd77a58a919dc6640a6989585669b4ea4aab07ce750500f09d5ca4ba7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c49k17nd/wheels/42/f1/aa/a01cebc65547819ce7e859ed78b46258e145fd7cdc29147efd\n",
            "Successfully built perlin-numpy\n",
            "Installing collected packages: perlin-numpy\n",
            "Successfully installed perlin-numpy-0.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grpcio==1.48.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_caWndRfB9j",
        "outputId": "d3b9455f-7c44-45fd-84ab-3e8f9751af4f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting grpcio==1.48.2\n",
            "  Downloading grpcio-1.48.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from grpcio==1.48.2) (1.15.0)\n",
            "Installing collected packages: grpcio\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.47.0\n",
            "    Uninstalling grpcio-1.47.0:\n",
            "      Successfully uninstalled grpcio-1.47.0\n",
            "Successfully installed grpcio-1.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow\n",
        "!pip install numpy==1.22.0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5sWRFieJdT1M",
        "outputId": "c34111f9-2e37-4e96-f507-c92c1ebc6588"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (22.12.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.24.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.34.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.22.0\n",
            "  Downloading numpy-1.22.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.8 MB 26.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.0\n",
            "    Uninstalling numpy-1.24.0:\n",
            "      Successfully uninstalled numpy-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigquery 3.3.6 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbf9hPRG7Pjr"
      },
      "source": [
        "#@title Import libraries\n",
        "from glob import glob\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import linspace\n",
        "import soundfile as sf\n",
        "import time\n",
        "import IPython\n",
        "import tensorflow as tf\n",
        "from perlin_numpy import (\n",
        "    generate_fractal_noise_2d, generate_perlin_noise_2d, \n",
        ")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdZmiOYdBVSq"
      },
      "source": [
        "#@title Hyperparameters \n",
        "learning_rate = 0.001 #@param {type:\"raw\"}\n",
        "num_epochs_to_train =  10#@param {type:\"integer\"}\n",
        "batch_size =  32#@param {type:\"integer\"}\n",
        "vector_dimension = 64 #@param {type:\"integer\"}\n",
        "\n",
        "hop=100               #hop size (window size = 4*hop)\n",
        "sr=44100              #sampling rate\n",
        "min_level_db=-100     #reference values to normalize data\n",
        "ref_level_db=20\n",
        "\n",
        "LEARNING_RATE = learning_rate\n",
        "BATCH_SIZE = batch_size\n",
        "EPOCHS = num_epochs_to_train\n",
        "VECTOR_DIM=vector_dimension\n",
        "\n",
        "shape=128           #length of time axis of split specrograms         \n",
        "spec_split=1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y2kIlpC7_lD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14420227-5abe-4b2b-cda4-c306a14b570c"
      },
      "source": [
        "#@title Waveform to Spectrogram conversion\n",
        "\n",
        "''' Decorsière, Rémi, Peter L. Søndergaard, Ewen N. MacDonald, and Torsten Dau. \n",
        "\"Inversion of auditory spectrograms, traditional spectrograms, and other envelope representations.\" \n",
        "IEEE/ACM Transactions on Audio, Speech, and Language Processing 23, no. 1 (2014): 46-56.'''\n",
        "\n",
        "#ORIGINAL CODE FROM https://github.com/yoyololicon/spectrogram-inversion\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import math\n",
        "import heapq\n",
        "from torchaudio.transforms import MelScale, Spectrogram\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "specobj = Spectrogram(n_fft=4*hop, win_length=4*hop, hop_length=hop, pad=0, power=2, normalized=False)\n",
        "specfunc = specobj.forward\n",
        "melobj = MelScale(n_mels=hop, sample_rate=sr, f_min=0.)\n",
        "melfunc = melobj.forward\n",
        "\n",
        "def melspecfunc(waveform):\n",
        "  specgram = specfunc(waveform)\n",
        "  mel_specgram = melfunc(specgram)\n",
        "  return mel_specgram\n",
        "\n",
        "def spectral_convergence(input, target):\n",
        "    return 20 * ((input - target).norm().log10() - target.norm().log10())\n",
        "\n",
        "def GRAD(spec, transform_fn, samples=None, init_x0=None, maxiter=1000, tol=1e-6, verbose=1, evaiter=10, lr=0.002):\n",
        "\n",
        "    spec = torch.Tensor(spec)\n",
        "    samples = (spec.shape[-1]*hop)-hop\n",
        "\n",
        "    if init_x0 is None:\n",
        "        init_x0 = spec.new_empty((1,samples)).normal_(std=1e-6)\n",
        "    x = nn.Parameter(init_x0)\n",
        "    T = spec\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam([x], lr=lr)\n",
        "\n",
        "    bar_dict = {}\n",
        "    metric_func = spectral_convergence\n",
        "    bar_dict['spectral_convergence'] = 0\n",
        "    metric = 'spectral_convergence'\n",
        "\n",
        "    init_loss = None\n",
        "    with tqdm(total=maxiter, disable=not verbose) as pbar:\n",
        "        for i in range(maxiter):\n",
        "            optimizer.zero_grad()\n",
        "            V = transform_fn(x)\n",
        "            loss = criterion(V, T)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr = lr*0.9999\n",
        "            for param_group in optimizer.param_groups:\n",
        "              param_group['lr'] = lr\n",
        "\n",
        "            if i % evaiter == evaiter - 1:\n",
        "                with torch.no_grad():\n",
        "                    V = transform_fn(x)\n",
        "                    bar_dict[metric] = metric_func(V, spec).item()\n",
        "                    l2_loss = criterion(V, spec).item()\n",
        "                    pbar.set_postfix(**bar_dict, loss=l2_loss)\n",
        "                    pbar.update(evaiter)\n",
        "\n",
        "    return x.detach().view(-1).cpu()\n",
        "\n",
        "def normalize(S):\n",
        "  return np.clip((((S - min_level_db) / -min_level_db)*2.)-1., -1, 1)\n",
        "\n",
        "def denormalize(S):\n",
        "  return (((np.clip(S, -1, 1)+1.)/2.) * -min_level_db) + min_level_db\n",
        "\n",
        "def prep(wv,hop=192):\n",
        "  S = np.array(torch.squeeze(melspecfunc(torch.Tensor(wv).view(1,-1))).detach().cpu())\n",
        "  S = librosa.power_to_db(S)-ref_level_db\n",
        "  return normalize(S)\n",
        "\n",
        "def deprep(S):\n",
        "  S = denormalize(S)+ref_level_db\n",
        "  S = librosa.db_to_power(S)\n",
        "  wv = GRAD(np.expand_dims(S,0), melspecfunc, maxiter=2500, evaiter=10, tol=1e-8)\n",
        "  return np.array(np.squeeze(wv))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (100) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNRYjsCDqDjF"
      },
      "source": [
        "#@title Helper functions\n",
        "\n",
        "#Generate spectrograms from waveform array\n",
        "def tospec(data):\n",
        "  specs=np.empty(data.shape[0], dtype=object)\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    S=prep(x)\n",
        "    S = np.array(S, dtype=np.float32)\n",
        "    specs[i]=np.expand_dims(S, -1)\n",
        "  print(specs.shape)\n",
        "  return specs\n",
        "\n",
        "#Generate multiple spectrograms with a determined length from single wav file\n",
        "def tospeclong(path, length=4*44100):\n",
        "  x, sr = librosa.load(path,sr=44100)\n",
        "  x,_ = librosa.effects.trim(x)\n",
        "  loudls = librosa.effects.split(x, top_db=50)\n",
        "  xls = np.array([])\n",
        "  for interv in loudls:\n",
        "    xls = np.concatenate((xls,x[interv[0]:interv[1]]))\n",
        "  x = xls\n",
        "  num = x.shape[0]//length\n",
        "  specs=np.empty(num, dtype=object)\n",
        "  for i in range(num-1):\n",
        "    a = x[i*length:(i+1)*length]\n",
        "    S = prep(a)\n",
        "    S = np.array(S, dtype=np.float32)\n",
        "    try:\n",
        "      sh = S.shape\n",
        "      specs[i]=S\n",
        "    except AttributeError:\n",
        "      print('spectrogram failed')\n",
        "  print(specs.shape)\n",
        "  return specs\n",
        "\n",
        "#Waveform array from path of folder containing wav files\n",
        "def audio_array(path):\n",
        "  ls = glob(f'{path}/*.wav')\n",
        "  adata = []\n",
        "  for i in range(len(ls)):\n",
        "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
        "    x = np.array(x, dtype=np.float32)\n",
        "    adata.append(x)\n",
        "  return np.array(adata)\n",
        "\n",
        "#Waveform array from path of folder containing wav files\n",
        "def single_audio_array(path):\n",
        "  ls = glob(path)\n",
        "  adata = []\n",
        "  for i in range(len(ls)):\n",
        "    x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
        "    x = np.array(x, dtype=np.float32)\n",
        "    adata.append(x)\n",
        "  return np.array(adata)\n",
        "\n",
        "\n",
        "#Concatenate spectrograms in array along the time axis\n",
        "def testass(a):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Split spectrograms in chunks with equal size\n",
        "def splitcut(data):\n",
        "  ls = []\n",
        "  mini = 0\n",
        "  minifinal = spec_split*shape   #max spectrogram length\n",
        "  for i in range(data.shape[0]-1):\n",
        "    if data[i].shape[1]<=data[i+1].shape[1]:\n",
        "      mini = data[i].shape[1]\n",
        "    else:\n",
        "      mini = data[i+1].shape[1]\n",
        "    if mini>=3*shape and mini<minifinal:\n",
        "      minifinal = mini\n",
        "  for i in range(data.shape[0]):\n",
        "    x = data[i]\n",
        "    if x.shape[1]>=3*shape:\n",
        "      for n in range(x.shape[1]//minifinal):\n",
        "        ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])\n",
        "      ls.append(x[:,-minifinal:,:])\n",
        "  return np.array(ls)\n",
        "\n",
        "# Generates timestamp string of \"day_month_year_hourMin\" \n",
        "def get_time_stamp():\n",
        "  secondsSinceEpoch = time.time()\n",
        "  timeObj = time.localtime(secondsSinceEpoch)\n",
        "  x = ('%d_%d_%d_%d%d' % (timeObj.tm_mday, timeObj.tm_mon, timeObj.tm_year, timeObj.tm_hour, timeObj.tm_min))\n",
        "  return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW7iOo_KIOBX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7B5LHi1-jRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b14e3d4-72c8-4850-cd0d-3d9592235e34"
      },
      "source": [
        "\n",
        "#@title Import folder containing .wav files for training\n",
        "\n",
        "#Generating Mel-Spectrogram dataset (Uncomment where needed)\n",
        "#adata: source spectrograms\n",
        "\n",
        "audio_directory = \"/content/drive/MyDrive/HCA_SONGS\" #@param {type:\"string\"}\n",
        "\n",
        "#AUDIO TO CONVERT\n",
        "awv = audio_array(audio_directory)         #get waveform array from folder containing wav files\n",
        "aspec = tospec(awv)                        #get spectrogram array\n",
        "adata = splitcut(aspec)                    #split spectrogams to fixed \n",
        "print(np.shape(adata))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-359c2297825e>:45: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(adata)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200,)\n",
            "(22600, 100, 128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "o41UJxFfhEfF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddE4uNQMxNCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7db6d93-7d74-45d7-e5e5-83124f45cb02"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "disable_eager_execution()\n",
        "\n",
        "\n",
        "def _calculate_reconstruction_loss(y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "\n",
        "def calculate_kl_loss(model):\n",
        "    # wrap `_calculate_kl_loss` such that it takes the model as an argument,\n",
        "    # returns a function which can take arbitrary number of arguments\n",
        "    # (for compatibility with `metrics` and utility in the loss function)\n",
        "    # and returns the kl loss\n",
        "    def _calculate_kl_loss(*args):\n",
        "        kl_loss = -0.5 * K.sum(1 + model.log_variance - K.square(model.mu) -\n",
        "                               K.exp(model.log_variance), axis=1)\n",
        "        return kl_loss\n",
        "    return _calculate_kl_loss\n",
        "\n",
        "\n",
        "class VAE:\n",
        "    \"\"\"\n",
        "    VAE represents a Deep Convolutional variational autoencoder architecture\n",
        "    with mirrored encoder and decoder components.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_shape,\n",
        "                 conv_filters,\n",
        "                 conv_kernels,\n",
        "                 conv_strides,\n",
        "                 latent_space_dim):\n",
        "        self.input_shape = input_shape # [28, 28, 1]\n",
        "        self.conv_filters = conv_filters # [2, 4, 8]\n",
        "        self.conv_kernels = conv_kernels # [3, 5, 3]\n",
        "        self.conv_strides = conv_strides # [1, 2, 2]\n",
        "        self.latent_space_dim = latent_space_dim # 2\n",
        "        self.reconstruction_loss_weight = 1000\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.model = None\n",
        "\n",
        "        self._num_conv_layers = len(conv_filters)\n",
        "        self._shape_before_bottleneck = None\n",
        "        self._model_input = None\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def summary(self):\n",
        "        self.encoder.summary()\n",
        "        self.decoder.summary()\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, learning_rate=0.0001):\n",
        "        adam = Adam(learning_rate=learning_rate)\n",
        "\n",
        "        self.model.compile(optimizer=adam,\n",
        "                           loss=self._calculate_combined_loss,\n",
        "                           metrics=[_calculate_reconstruction_loss,\n",
        "                                    calculate_kl_loss(self)])\n",
        "\n",
        "    def train(self, x_train, batch_size, num_epochs):\n",
        "        self.model.fit(x_train,\n",
        "                       x_train,\n",
        "                       batch_size=batch_size,\n",
        "                       epochs=num_epochs,\n",
        "                       shuffle=True)\n",
        "\n",
        "    def save(self, save_folder=\".\"):\n",
        "        self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "        self._save_parameters(save_folder)\n",
        "        self._save_weights(save_folder)\n",
        "\n",
        "    def load_weights(self, weights_path):\n",
        "        self.model.load_weights(weights_path)\n",
        "\n",
        "    def reconstruct(self, images):\n",
        "        latent_representations = self.encoder.predict(images)\n",
        "        reconstructed_images = self.decoder.predict(latent_representations)\n",
        "        return reconstructed_images, latent_representations\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, save_folder=\".\"):\n",
        "        parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(parameters_path, \"rb\") as f:\n",
        "            parameters = pickle.load(f)\n",
        "        autoencoder = VAE(*parameters)\n",
        "        weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        autoencoder.load_weights(weights_path)\n",
        "        return autoencoder\n",
        "\n",
        "    def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "        reconstruction_loss = _calculate_reconstruction_loss(y_target, y_predicted)\n",
        "        kl_loss = calculate_kl_loss(self)()\n",
        "        combined_loss = self.reconstruction_loss_weight * reconstruction_loss\\\n",
        "                                                         + kl_loss\n",
        "        return combined_loss\n",
        "\n",
        "    def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "\n",
        "    def _save_parameters(self, save_folder):\n",
        "        parameters = [\n",
        "            self.input_shape,\n",
        "            self.conv_filters,\n",
        "            self.conv_kernels,\n",
        "            self.conv_strides,\n",
        "            self.latent_space_dim\n",
        "        ]\n",
        "        save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            pickle.dump(parameters, f)\n",
        "\n",
        "    def _save_weights(self, save_folder):\n",
        "        save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "        self.model.save_weights(save_path)\n",
        "\n",
        "    def _build(self):\n",
        "        self._build_encoder()\n",
        "        self._build_decoder()\n",
        "        self._build_autoencoder()\n",
        "\n",
        "    def _build_autoencoder(self):\n",
        "        model_input = self._model_input\n",
        "        model_output = self.decoder(self.encoder(model_input))\n",
        "        self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "    def _build_decoder(self):\n",
        "        decoder_input = self._add_decoder_input()\n",
        "        dense_layer = self._add_dense_layer(decoder_input)\n",
        "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "        self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "    def _add_decoder_input(self):\n",
        "        return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "    def _add_dense_layer(self, decoder_input):\n",
        "        num_neurons = np.prod(self._shape_before_bottleneck) # [1, 2, 4] -> 8\n",
        "        dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "        return dense_layer\n",
        "\n",
        "    def _add_reshape_layer(self, dense_layer):\n",
        "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "    def _add_conv_transpose_layers(self, x):\n",
        "        \"\"\"Add conv transpose blocks.\"\"\"\n",
        "        # loop through all the conv layers in reverse order and stop at the\n",
        "        # first layer\n",
        "        for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "            x = self._add_conv_transpose_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_transpose_layer(self, layer_index, x):\n",
        "        layer_num = self._num_conv_layers - layer_index\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "        x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_decoder_output(self, x):\n",
        "        conv_transpose_layer = Conv2DTranspose(\n",
        "            filters=1,\n",
        "            kernel_size=self.conv_kernels[0],\n",
        "            strides=self.conv_strides[0],\n",
        "            padding=\"same\",\n",
        "            name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "        )\n",
        "        x = conv_transpose_layer(x)\n",
        "        output_layer = Activation(\"sigmoid\", name=\"sigmoid_layer\")(x)\n",
        "        return output_layer\n",
        "\n",
        "    def _build_encoder(self):\n",
        "        encoder_input = self._add_encoder_input()\n",
        "        conv_layers = self._add_conv_layers(encoder_input)\n",
        "        bottleneck = self._add_bottleneck(conv_layers)\n",
        "        self._model_input = encoder_input\n",
        "        self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "    def _add_encoder_input(self):\n",
        "        return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "    def _add_conv_layers(self, encoder_input):\n",
        "        \"\"\"Create all convolutional blocks in encoder.\"\"\"\n",
        "        x = encoder_input\n",
        "        for layer_index in range(self._num_conv_layers):\n",
        "            x = self._add_conv_layer(layer_index, x)\n",
        "        return x\n",
        "\n",
        "    def _add_conv_layer(self, layer_index, x):\n",
        "        \"\"\"Add a convolutional block to a graph of layers, consisting of\n",
        "        conv 2d + ReLU + batch normalization.\n",
        "        \"\"\"\n",
        "        layer_number = layer_index + 1\n",
        "        conv_layer = Conv2D(\n",
        "            filters=self.conv_filters[layer_index],\n",
        "            kernel_size=self.conv_kernels[layer_index],\n",
        "            strides=self.conv_strides[layer_index],\n",
        "            padding=\"same\",\n",
        "            name=f\"encoder_conv_layer_{layer_number}\"\n",
        "        )\n",
        "        x = conv_layer(x)\n",
        "        x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "        x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "        return x\n",
        "\n",
        "    def _add_bottleneck(self, x):\n",
        "        \"\"\"Flatten data and add bottleneck with Guassian sampling (Dense\n",
        "        layer).\n",
        "        \"\"\"\n",
        "        self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "        x = Flatten()(x)\n",
        "        self.mu = Dense(self.latent_space_dim, name=\"mu\")(x)\n",
        "        self.log_variance = Dense(self.latent_space_dim,\n",
        "                                  name=\"log_variance\")(x)\n",
        "\n",
        "        def sample_point_from_normal_distribution(args):\n",
        "            mu, log_variance = args\n",
        "            epsilon = K.random_normal(shape=K.shape(self.mu), mean=0.,\n",
        "                                      stddev=1.)\n",
        "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "            return sampled_point\n",
        "\n",
        "        x = Lambda(sample_point_from_normal_distribution,\n",
        "                   name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "        return x\n",
        "\n",
        "print(\"VAE successfully built\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAE successfully built\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRJI_kQsHgXP"
      },
      "source": [
        "#@title Training functions\n",
        "\n",
        "def train(x_train, learning_rate, batch_size, epochs): \n",
        "  vae = VAE(\n",
        "      input_shape = (100, 128, 1),\n",
        "      conv_filters=(512, 256, 128, 64, 32),\n",
        "      conv_kernels=(3, 3, 3, 3, 3),\n",
        "      conv_strides=(2, 2, 2, 2, (2,1)),\n",
        "      latent_space_dim = VECTOR_DIM\n",
        "  )\n",
        "  vae.summary()\n",
        "  vae.compile(learning_rate)\n",
        "  vae.train(x_train, batch_size, epochs)\n",
        "  return vae\n",
        "\n",
        "def train_tfdata(x_train, learning_rate, epochs=10): \n",
        "  vae = VAE(\n",
        "      input_shape = (hop, 1*shape, 1),\n",
        "      conv_filters=(512, 256, 128, 64, 32),\n",
        "      conv_kernels=(3, 3, 3, 3, 3),\n",
        "      conv_strides=(2, 2, 2, 2, (2,1)),\n",
        "      latent_space_dim = VECTOR_DIM\n",
        "  )\n",
        "  vae.summary()\n",
        "  vae.compile(learning_rate)\n",
        "  vae.train(x_train, num_epochs=epochs)\n",
        "  return vae\n",
        "\n",
        "def continue_training(checkpoint):\n",
        "  vae = VAE.load(checkpoint)\n",
        "  vae.summary()\n",
        "  vae.compile(LEARNING_RATE)\n",
        "  vae.train(adata,BATCH_SIZE,EPOCHS)\n",
        "  return vae\n",
        "\n",
        "def load_model(checkpoint):\n",
        "  vae = VAE.load(checkpoint)\n",
        "  vae.summary()\n",
        "  vae.compile(LEARNING_RATE)\n",
        "  return vae\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yzHZt-PJc65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71df28ac-a045-433b-e0ae-c9369aeea507"
      },
      "source": [
        "#@title Start training from scratch or resume training\n",
        "\n",
        "training_run_name = \"/content/drive\" #@param {type:\"string\"}\n",
        "checkpoint_save_directory = \"/path/to/your/checkpoints/\" #@param {type:\"string\"}\n",
        "resume_training = False #@param {type:\"boolean\"}\n",
        "resume_training_checkpoint_path = \"/path/to/your/checkpoints/\" #@param {type:\"string\"}\n",
        "\n",
        "current_time = get_time_stamp()\n",
        "\n",
        "if not resume_training:\n",
        "  vae = train(adata, LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
        " #vae = train_tfdata(dsa, LEARNING_RATE, EPOCHS)\n",
        "  vae.save(f\"{checkpoint_save_directory}{training_run_name}_{current_time}_h{hop}_w{shape}_z{VECTOR_DIM}\")\n",
        "else:\n",
        "  vae = continue_training(resume_training_checkpoint_path)\n",
        "  vae.save(f\"{checkpoint_save_directory}{training_run_name}_{current_time}_h{hop}_w{shape}_z{VECTOR_DIM}\")\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, 100, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " encoder_conv_layer_1 (Conv2D)  (None, 50, 64, 512)  5120        ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " encoder_relu_1 (ReLU)          (None, 50, 64, 512)  0           ['encoder_conv_layer_1[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_bn_1 (BatchNormalizati  (None, 50, 64, 512)  2048       ['encoder_relu_1[0][0]']         \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_layer_2 (Conv2D)  (None, 25, 32, 256)  1179904     ['encoder_bn_1[0][0]']           \n",
            "                                                                                                  \n",
            " encoder_relu_2 (ReLU)          (None, 25, 32, 256)  0           ['encoder_conv_layer_2[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_bn_2 (BatchNormalizati  (None, 25, 32, 256)  1024       ['encoder_relu_2[0][0]']         \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_layer_3 (Conv2D)  (None, 13, 16, 128)  295040      ['encoder_bn_2[0][0]']           \n",
            "                                                                                                  \n",
            " encoder_relu_3 (ReLU)          (None, 13, 16, 128)  0           ['encoder_conv_layer_3[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_bn_3 (BatchNormalizati  (None, 13, 16, 128)  512        ['encoder_relu_3[0][0]']         \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_layer_4 (Conv2D)  (None, 7, 8, 64)     73792       ['encoder_bn_3[0][0]']           \n",
            "                                                                                                  \n",
            " encoder_relu_4 (ReLU)          (None, 7, 8, 64)     0           ['encoder_conv_layer_4[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_bn_4 (BatchNormalizati  (None, 7, 8, 64)    256         ['encoder_relu_4[0][0]']         \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_conv_layer_5 (Conv2D)  (None, 4, 8, 32)     18464       ['encoder_bn_4[0][0]']           \n",
            "                                                                                                  \n",
            " encoder_relu_5 (ReLU)          (None, 4, 8, 32)     0           ['encoder_conv_layer_5[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_bn_5 (BatchNormalizati  (None, 4, 8, 32)    128         ['encoder_relu_5[0][0]']         \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " flatten_13 (Flatten)           (None, 1024)         0           ['encoder_bn_5[0][0]']           \n",
            "                                                                                                  \n",
            " mu (Dense)                     (None, 64)           65600       ['flatten_13[0][0]']             \n",
            "                                                                                                  \n",
            " log_variance (Dense)           (None, 64)           65600       ['flatten_13[0][0]']             \n",
            "                                                                                                  \n",
            " encoder_output (Lambda)        (None, 64)           0           ['mu[0][0]',                     \n",
            "                                                                  'log_variance[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,707,488\n",
            "Trainable params: 1,705,504\n",
            "Non-trainable params: 1,984\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " decoder_input (InputLayer)  [(None, 64)]              0         \n",
            "                                                                 \n",
            " decoder_dense (Dense)       (None, 1024)              66560     \n",
            "                                                                 \n",
            " reshape_13 (Reshape)        (None, 4, 8, 32)          0         \n",
            "                                                                 \n",
            " decoder_conv_transpose_laye  (None, 8, 8, 32)         9248      \n",
            " r_1 (Conv2DTranspose)                                           \n",
            "                                                                 \n",
            " decoder_relu_1 (ReLU)       (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " decoder_bn_1 (BatchNormaliz  (None, 8, 8, 32)         128       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " decoder_conv_transpose_laye  (None, 16, 16, 64)       18496     \n",
            " r_2 (Conv2DTranspose)                                           \n",
            "                                                                 \n",
            " decoder_relu_2 (ReLU)       (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " decoder_bn_2 (BatchNormaliz  (None, 16, 16, 64)       256       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " decoder_conv_transpose_laye  (None, 32, 32, 128)      73856     \n",
            " r_3 (Conv2DTranspose)                                           \n",
            "                                                                 \n",
            " decoder_relu_3 (ReLU)       (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " decoder_bn_3 (BatchNormaliz  (None, 32, 32, 128)      512       \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " decoder_conv_transpose_laye  (None, 64, 64, 256)      295168    \n",
            " r_4 (Conv2DTranspose)                                           \n",
            "                                                                 \n",
            " decoder_relu_4 (ReLU)       (None, 64, 64, 256)       0         \n",
            "                                                                 \n",
            " decoder_bn_4 (BatchNormaliz  (None, 64, 64, 256)      1024      \n",
            " ation)                                                          \n",
            "                                                                 \n",
            " decoder_conv_transpose_laye  (None, 128, 128, 1)      2305      \n",
            " r_5 (Conv2DTranspose)                                           \n",
            "                                                                 \n",
            " sigmoid_layer (Activation)  (None, 128, 128, 1)       0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 467,553\n",
            "Trainable params: 466,593\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n",
            "Model: \"autoencoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_input (InputLayer)  [(None, 100, 128, 1)]     0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 64)                1707488   \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 128, 128, 1)       467553    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,175,041\n",
            "Trainable params: 2,172,097\n",
            "Non-trainable params: 2,944\n",
            "_________________________________________________________________\n",
            "Train on 22600 samples\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-a7c10b58b7e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresume_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m  \u001b[0;31m#vae = train_tfdata(dsa, LEARNING_RATE, EPOCHS)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{checkpoint_save_directory}{training_run_name}_{current_time}_h{hop}_w{shape}_z{VECTOR_DIM}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-ff6f33239ad1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-7c4f1d24d3ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         self.model.fit(x_train,\n\u001b[0m\u001b[1;32m     79\u001b[0m                        \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         return func.fit(\n\u001b[0m\u001b[1;32m    855\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return fit_loop(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4579\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4581\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4583\u001b[0m         output_structure = tf.nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1481\u001b[0;31m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[1;32m   1482\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m                                                run_metadata_ptr)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) INVALID_ARGUMENT: required broadcastable shapes\n\t [[{{node loss_8/decoder_loss/sub}}]]\n\t [[Func/training_9/Adam/gradients/gradients/decoder_13/decoder_bn_3/cond_grad/StatelessIf/then/_7504/input/_18702/_4897]]\n  (1) INVALID_ARGUMENT: required broadcastable shapes\n\t [[{{node loss_8/decoder_loss/sub}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWqhd1LILIxD"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ceWvu0rMPCGx"
      },
      "source": [
        "#@title Build VAE Neural Network\n",
        "\n",
        "#VAE\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class VAE:\n",
        "  \"\"\"\n",
        "  VAE represents a Deep Convolutional autoencoder architecture\n",
        "  with mirrored encoder and decoder components.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_shape, #shape of the input data\n",
        "               conv_filters, #convolutional network filters\n",
        "               conv_kernels, #convNet kernel size\n",
        "               conv_strides, #convNet strides\n",
        "               latent_space_dim):\n",
        "    self.input_shape = input_shape # [28, 28, 1], in this case is 28 x 28 pixels on 1 channel for greyscale\n",
        "    self.conv_filters = conv_filters # is a list for each layer, i.e. [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # list of kernels per layer, [1,2,3]\n",
        "    self.conv_strides = conv_strides # stride for each filter [1, 2, 2], note: 2 means you are downsampling the data in half\n",
        "    self.latent_space_dim = latent_space_dim # how many neurons on bottleneck\n",
        "    self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "\n",
        "    self._num_conv_layers = len(conv_filters)\n",
        "    self._shape_before_bottleneck = None\n",
        "    self._model_input = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.decoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.model.summary()\n",
        "\n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    self._build_autoencoder()\n",
        "\n",
        "  def compile(self, learning_rate=0.0001):\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    self.model.compile(optimizer=optimizer, loss=self._calculate_combined_loss,\n",
        "                       metrics=[self._calculate_reconstruction_loss,\n",
        "                                self._calculate_kl_loss])\n",
        "\n",
        "  def train(self, x_train, batch_size, num_epochs):\n",
        "   # checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
        "   #                              save_best_only=True, mode='auto', period=1)\n",
        "    self.model.fit(x_train,\n",
        "                   x_train,\n",
        "                   batch_size=batch_size,\n",
        "                   epochs=num_epochs,\n",
        "                   shuffle=True)\n",
        "                   #callbacks=[checkpoint])\n",
        "\n",
        "  def save(self, save_folder=\".\"):\n",
        "    self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "    self._save_parameters(save_folder)\n",
        "    self._save_weights(save_folder)\n",
        "\n",
        "  def load_weights(self, weights_path):\n",
        "    self.model.load_weights(weights_path)\n",
        "\n",
        "  def reconstruct(self, spec):\n",
        "      latent_representations = self.encoder.predict(spec)\n",
        "      reconstructed_spec = self.decoder.predict(latent_representations)\n",
        "      return reconstructed_spec, latent_representations\n",
        "  \n",
        "  def encode(self, spec):\n",
        "    latent_representation = self.encoder.predict(spec)\n",
        "    return latent_representation\n",
        "  \n",
        "  def sample_from_latent_space(self, z):\n",
        "      z_vector = self.decoder.predict(z)\n",
        "      return z_vector\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, save_folder=\".\"):\n",
        "      parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(parameters_path, \"rb\") as f:\n",
        "          parameters = pickle.load(f)\n",
        "      autoencoder = VAE(*parameters)\n",
        "      weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      autoencoder.load_weights(weights_path)\n",
        "      return autoencoder\n",
        "\n",
        "  def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "    reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "    kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "    combined_loss = self.reconstruction_loss_weight * reconstruction_loss + kl_loss\n",
        "    return combined_loss\n",
        "  \n",
        "  def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "  def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "    kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                           K.exp(self.log_variance), axis =1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "      if not os.path.exists(folder):\n",
        "          os.makedirs(folder)\n",
        "\n",
        "  def _save_parameters(self, save_folder):\n",
        "      parameters = [\n",
        "          self.input_shape,\n",
        "          self.conv_filters,\n",
        "          self.conv_kernels,\n",
        "          self.conv_strides,\n",
        "          self.latent_space_dim\n",
        "      ]\n",
        "      save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(parameters, f)\n",
        "\n",
        "  def _save_weights(self, save_folder):\n",
        "      save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      self.model.save_weights(save_path)\n",
        "\n",
        "#-----------AUTOENCODER----------#\n",
        "\n",
        "  def _build_autoencoder(self):\n",
        "    model_input = self._model_input\n",
        "    model_output = self.decoder(self.encoder(model_input))\n",
        "    self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "#--------------DECODER------------#\n",
        "\n",
        "  def _build_decoder(self):\n",
        "    decoder_input = self._add_decoder_input()\n",
        "    dense_layer = self._add_dense_layer(decoder_input)\n",
        "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "    decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "    self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "  def _add_decoder_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "  def _add_dense_layer(self, decoder_input):\n",
        "    num_neurons = np.prod(self._shape_before_bottleneck) # [ 1, 2, 4] -> 8\n",
        "    dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "    return dense_layer\n",
        "\n",
        "  def _add_reshape_layer(self, dense_layer):\n",
        "    return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "  def _add_conv_transpose_layers(self, x):\n",
        "    \"\"\"Add conv transpose blocks.\"\"\"\n",
        "    # Loop through all the conv layers in reverse order and\n",
        "    # stop at the first layer\n",
        "    for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "      x = self._add_conv_transpose_layer(layer_index, x)\n",
        "    return x\n",
        "\n",
        "  def _add_conv_transpose_layer(self, layer_index, x):\n",
        "    layer_num = self._num_conv_layers - layer_index\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters=self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "    x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  def _add_decoder_output(self, x):\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters = 1,\n",
        "        kernel_size = self.conv_kernels[0],\n",
        "        strides = self.conv_strides[0],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    output_layer = Activation(\"sigmoid\", name=\"sigmoid_output_layer\")(x)\n",
        "    return output_layer\n",
        "\n",
        "#----------------ENCODER-----------------#\n",
        "\n",
        "  def _build_encoder(self):\n",
        "    encoder_input = self._add_encoder_input()\n",
        "    conv_layers = self._add_conv_layers(encoder_input)\n",
        "    bottleneck =  self._add_bottleneck(conv_layers)\n",
        "    self._model_input = encoder_input\n",
        "    self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "  def _add_encoder_input(self):\n",
        "    return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "  def _add_conv_layers(self, encoder_input):\n",
        "    \"\"\"Creates all convolutional blocks in encoder\"\"\"\n",
        "    x = encoder_input\n",
        "    for layer_index in range(self._num_conv_layers):\n",
        "      x = self._add_conv_layer(layer_index, x)\n",
        "    return x\n",
        "  \n",
        "  def _add_conv_layer(self, layer_index, x):\n",
        "    \"\"\"Adds a convolutional block to a graph of layers, consisting\n",
        "    of Conv 2d + ReLu activation + batch normalization.\n",
        "    \"\"\"\n",
        "    layer_number = layer_index + 1\n",
        "    conv_layer = Conv2D(\n",
        "        filters= self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name = f\"encoder_conv_layer_{layer_number}\"\n",
        "    )\n",
        "    x = conv_layer(x)\n",
        "    x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "    x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "    return x\n",
        "\n",
        "#-------------Bottleneck (Latent Space)-------------#\n",
        "\n",
        "  def _add_bottleneck(self, x):\n",
        "    \"\"\"Flatten data and add bottleneck with Gaussian sampling (Dense layer)\"\"\"\n",
        "    self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.latent_space_dim,name=\"mu\")(x)\n",
        "    self.log_variance = Dense(self.latent_space_dim,\n",
        "                              name=\"log_variance\")(x)\n",
        "    \n",
        "    def sample_point_from_normal_distribution(args):\n",
        "      mu, log_variance = args\n",
        "      epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., \n",
        "                                stddev=1.)\n",
        "      sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "\n",
        "      return sampled_point\n",
        "\n",
        "    x = Lambda(sample_point_from_normal_distribution, \n",
        "               name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "    return x\n",
        "\n",
        "print(\"VAE successfully built\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe62172WPEYD",
        "cellView": "form"
      },
      "source": [
        "#@title Load Checkpoint for Generating\n",
        "\n",
        "checkpoint_load_directory = \"/path/to/your/checkpoint\" #@param {type:\"string\"}\n",
        "\n",
        "#-------LOAD MODEL FOR GENERATING-------------#\n",
        "\n",
        "vae = VAE.load(checkpoint_load_directory)\n",
        "print(\"Loaded checkpoint\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-pa9kR6Pe8B",
        "cellView": "form"
      },
      "source": [
        "#@title Import synthesis utility functions\n",
        "\n",
        "#-----TESTING FUNCTIONS ----------- #\n",
        "\n",
        "def select_spec(spec, labels, num_spec=10):\n",
        "    sample_spec_index = np.random.choice(range(len(spec)), num_spec)\n",
        "    sample_spec = spec[sample_spec_index]\n",
        "    sample_labels = labels[sample_spec_index]\n",
        "    return sample_spec, sample_labels\n",
        "\n",
        "\n",
        "def plot_reconstructed_spec(spec, reconstructed_spec):\n",
        "    fig = plt.figure(figsize=(15, 3))\n",
        "    num_spec = len(spec)\n",
        "    for i, (image, reconstructed_image) in enumerate(zip(spec, reconstructed_spec)):\n",
        "        image = image.squeeze()\n",
        "        ax = fig.add_subplot(2, num_spec, i + 1)\n",
        "        ax.axis(\"off\")\n",
        "        ax.imshow(image, cmap=\"gray_r\")\n",
        "        reconstructed_image = reconstructed_image.squeeze()\n",
        "        ax = fig.add_subplot(2, num_spec, i + num_spec + 1)\n",
        "        ax.axis(\"off\")\n",
        "        ax.imshow(reconstructed_image, cmap=\"gray_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_spec_encoded_in_latent_space(latent_representations, sample_labels):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(latent_representations[:, 0],\n",
        "                latent_representations[:, 1],\n",
        "                cmap=\"rainbow\",\n",
        "                c=sample_labels,\n",
        "                alpha=0.5,\n",
        "                s=2)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "#---------------NOISE GENERATOR FUNCTIONS ------------#\n",
        "\n",
        "def generate_random_z_vect(seed=1001,size_z=1,scale=1.0):\n",
        "    np.random.seed(seed)\n",
        "    x = np.random.uniform(low=(scale * -1.0), high=scale, size=(size_z,VECTOR_DIM))\n",
        "    return x\n",
        "\n",
        "def generate_z_vect_from_perlin_noise(seed=1001, size_z=1, scale=1.0):\n",
        "    np.random.seed(seed)\n",
        "    x = generate_perlin_noise_2d((size_z, VECTOR_DIM), (1,1))\n",
        "    x = x*scale\n",
        "    return x\n",
        "\n",
        "def generate_z_vect_from_fractal_noise(seed=1001, size_z=1, scale=1.0,):\n",
        "    np.random.seed(seed)\n",
        "    x = generate_fractal_noise_2d((size_z, VECTOR_DIM), (1,1),)\n",
        "    x = x*scale\n",
        "    return x\n",
        "\n",
        "\n",
        "#-------SPECTROGRAM AND SOUND SYNTHESIS UTILITY FUNCTIONS -------- #\n",
        "\n",
        "#Assembling generated Spectrogram chunks into final Spectrogram\n",
        "def specass(a,spec):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim-1):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  diff = spec.shape[1]-(nim*shape)\n",
        "  a = np.squeeze(a)\n",
        "  con = np.concatenate((con,a[-1,:,-diff:]), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Splitting input spectrogram into different chunks to feed to the generator\n",
        "def chopspec(spec):\n",
        "  dsa=[]\n",
        "  for i in range(spec.shape[1]//shape):\n",
        "    im = spec[:,i*shape:i*shape+shape]\n",
        "    im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
        "    dsa.append(im)\n",
        "  imlast = spec[:,-shape:]\n",
        "  imlast = np.reshape(imlast, (imlast.shape[0],imlast.shape[1],1))\n",
        "  dsa.append(imlast)\n",
        "  return np.array(dsa, dtype=np.float32)\n",
        "\n",
        "#Converting from source Spectrogram to target Spectrogram\n",
        "def towave_reconstruct(spec, spec1, name, path='../content/', show=False, save=False):\n",
        "  specarr = chopspec(spec)\n",
        "  specarr1 = chopspec(spec1)\n",
        "  print(specarr.shape)\n",
        "  a = specarr\n",
        "  print('Generating...')\n",
        "  ab = specarr1\n",
        "  print('Assembling and Converting...')\n",
        "  a = specass(a,spec)\n",
        "  ab = specass(ab,spec1)\n",
        "  awv = deprep(a)\n",
        "  abwv = deprep(ab)\n",
        "  if save:\n",
        "    print('Saving...')\n",
        "    pathfin = f'{path}/{name}'\n",
        "    sf.write(f'{pathfin}.wav', awv, sr)\n",
        "    print('Saved WAV!')\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  if show:\n",
        "    fig, axs = plt.subplots(ncols=2)\n",
        "    axs[0].imshow(np.flip(a, -2), cmap=None)\n",
        "    axs[0].axis('off')\n",
        "    axs[0].set_title('Reconstructed')\n",
        "    axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
        "    axs[1].axis('off')\n",
        "    axs[1].set_title('Input')\n",
        "    plt.show()\n",
        "  return abwv\n",
        "\n",
        "#Converting from Z vector generated spectrogram to waveform\n",
        "def towave_from_z(spec, name, path='../content/', show=False, save=False):\n",
        "  specarr = chopspec(spec)\n",
        "  print(specarr.shape)\n",
        "  a = specarr\n",
        "  print('Generating...')\n",
        "  print('Assembling and Converting...')\n",
        "  a = specass(a,spec)\n",
        "  awv = deprep(a)\n",
        "  if save:\n",
        "    print('Saving...')\n",
        "    pathfin = f'{path}/{name}'\n",
        "    sf.write(f'{pathfin}.wav', awv, sr)\n",
        "    print('Saved WAV!')\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  if show:\n",
        "    fig, axs = plt.subplots(ncols=1)\n",
        "    axs.imshow(np.flip(a, -2), cmap=None)\n",
        "    axs.axis('off')\n",
        "    axs.set_title('Decoder Synthesis')\n",
        "    plt.show()\n",
        "  return awv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HfNl_oQQDQ-",
        "cellView": "form"
      },
      "source": [
        "#@title Compare resynthesized MelSpec with ground truth MelSpec. Note: Only works if you imported an audio dataset\n",
        "\n",
        "num_spec_to_resynthesize =  5 #@param {type:\"integer\"}\n",
        "\n",
        "num_sample_spec_to_show = num_spec_to_resynthesize\n",
        "sample_spec, _ = select_spec(adata, adata, num_sample_spec_to_show)\n",
        "reconstructed_spec, _ = vae.reconstruct(sample_spec)\n",
        "plot_reconstructed_spec(sample_spec, reconstructed_spec)\n",
        "\n",
        "reconst = num_sample_spec_to_show\n",
        "\n",
        "for i in range(reconst):\n",
        "  y = towave_reconstruct(reconstructed_spec[i],sample_spec[i],name='reconstructions',show=True, save=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwHuwGDb0zvD",
        "cellView": "form"
      },
      "source": [
        "#@title Generate one-shot samples from latent space with random or manual seed\n",
        "num_samples_to_generate =   10#@param {type:\"integer\"}\n",
        "use_seed = False #@param {type:\"boolean\"}\n",
        "seed =  0 #@param {type:\"slider\", min:0, max:4294967295, step:1}\n",
        "scale_z_vectors =  1.5 #@param {type:\"slider\", min:-5.0, max:5.0, step:0.1}\n",
        "save_audio = False #@param {type:\"boolean\"}\n",
        "audio_name = \"one_shot\" #@param {type:\"string\"}\n",
        "audio_save_directory = \"/content/\" #@param {type:\"string\"}\n",
        "\n",
        "y = np.random.randint(0, 2**32-1)                          # generated random int to pass and convert into vector\n",
        "i=0\n",
        "while i < num_samples_to_generate:\n",
        "  if not use_seed:\n",
        "    z = generate_random_z_vect(y, num_samples_to_generate,scale=scale_z_vectors)  \n",
        "  else:\n",
        "    z = generate_random_z_vect(seed, num_samples_to_generate,scale=scale_z_vectors)\n",
        "  z_sample = np.array(vae.sample_from_latent_space(z))\n",
        "  towave_from_z(z_sample[i], name=f'{audio_name}_{i}',path=audio_save_directory,show=True, save=save_audio)\n",
        "  i+=1\n",
        "\n",
        "if not use_seed:\n",
        "  print(\"Generated from seed:\", y)\n",
        "else:\n",
        "  print(\"Generated from seed:\", seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8akqOemGJTIG",
        "cellView": "form"
      },
      "source": [
        "#@title Generate arbitrary long audio from latent space with random or custom seed using uniform, Perlin or fractal noise\n",
        "num_seeds_to_generate = 32#@param {type:\"integer\"}\n",
        "noise_type = \"uniform\" #@param [\"uniform\", \"perlin\", \"fractal\"]\n",
        "use_seed = False #@param {type:\"boolean\"}\n",
        "seed =  0 #@param {type:\"slider\", min:0, max:4294967295, step:1}\n",
        "scale_z_vectors =  1.5 #@param {type:\"slider\", min:-5.0, max:5.0, step:0.1}\n",
        "save_audio = False #@param {type:\"boolean\"}\n",
        "audio_name = \"VAE_synthesis2\" #@param {type:\"string\"}\n",
        "audio_save_directory = \"/content\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "y = np.random.randint(0, 2**32-1)                         # generated random int to pass and convert into vector\n",
        "if not use_seed:\n",
        "  if noise_type == \"uniform\":\n",
        "    z = generate_random_z_vect(y, num_seeds_to_generate,scale_z_vectors)            # vectors to input into latent space\n",
        "  if noise_type == \"perlin\":\n",
        "    z = generate_z_vect_from_perlin_noise(y, num_seeds_to_generate,scale_z_vectors)            # vectors to input into latent space\n",
        "  if noise_type == \"fractal\":\n",
        "    z = generate_z_vect_from_fractal_noise(y, num_seeds_to_generate,scale_z_vectors)            # vectors to input into latent space\n",
        "if use_seed:\n",
        "  if noise_type == \"uniform\":\n",
        "    z = generate_random_z_vect(seed, num_seeds_to_generate,scale_z_vectors)            # vectors to input into latent space\n",
        "  if noise_type == \"perlin\":\n",
        "    z = generate_z_vect_from_perlin_noise(seed, num_seeds_to_generate,scale_z_vectors)            # vectors to input into latent space\n",
        "  if noise_type == \"fractal\":\n",
        "    z = generate_z_vect_from_fractal_noise(seed, num_seeds_to_generate,scale_z_vectors)            # vectors to input into latent space\n",
        "z_sample = np.array(vae.sample_from_latent_space(z))\n",
        "assembled_spec = testass(z_sample)\n",
        "towave_from_z(assembled_spec,audio_name,audio_save_directory,show=True,save=save_audio)\n",
        "\n",
        "if not use_seed:\n",
        "  print(\"Generated from seed:\", y)\n",
        "else:\n",
        "  print(\"Generated from seed:\", seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dcnq9i8UcC4g"
      },
      "source": [
        "#@title Interpolate between two seeds for n-amount of steps\n",
        "\n",
        "use_seed = False #@param {type:\"boolean\"}\n",
        "seed_a =  0 #@param {type:\"slider\", min:0, max:4294967295, step:1}\n",
        "seed_b =  4294967295 #@param {type:\"slider\", min:0, max:4294967295, step:1}\n",
        "num_interpolation_steps =   8#@param {type:\"integer\"}\n",
        "scale_z_vectors =  1.5 #@param {type:\"slider\", min:-5.0, max:5.0, step:0.1}\n",
        "scale_interpolation_ratio =  1 #@param {type:\"slider\", min:-5.0, max:5.0, step:0.1}\n",
        "save_audio = False #@param {type:\"boolean\"}\n",
        "audio_name = \"random_seeds_interpolation\" #@param {type:\"string\"}\n",
        "audio_save_directory = \"/content/\" #@param {type:\"string\"}\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn z_input\n",
        " \n",
        "# uniform interpolation between two points in latent space\n",
        "def interpolate_points(p1, p2,scale, n_steps=10):\n",
        "\t# interpolate ratios between the points\n",
        "\tratios = linspace(-scale, scale, num=n_steps)\n",
        "\t# linear interpolate vectors\n",
        "\tvectors = list()\n",
        "\tfor ratio in ratios:\n",
        "\t\tv = (1.0 - ratio) * p1 + ratio * p2\n",
        "\t\tvectors.append(v)\n",
        "\treturn asarray(vectors)\n",
        "\n",
        "y = np.random.randint(0, 2**32-1)\n",
        "\n",
        "if not use_seed:\n",
        "  pts = generate_random_z_vect(y,10,scale_z_vectors)\n",
        "  interpolated = interpolate_points(pts[0], pts[1], scale_interpolation_ratio, num_interpolation_steps) # interpolate points in latent space\n",
        "else:\n",
        "  pts_a = generate_random_z_vect(seed_a,10,scale_z_vectors)\n",
        "  pts_b = generate_random_z_vect(seed_b,10,scale_z_vectors)\n",
        "  interpolated = interpolate_points(pts_a[0], pts_b[0], scale_interpolation_ratio, num_interpolation_steps) # interpolate points in latent space\n",
        "\n",
        "interp = np.array(vae.sample_from_latent_space(interpolated))\n",
        "assembled_spec = testass(interp)\n",
        "towave_from_z(assembled_spec,audio_name,audio_save_directory,show=True, save=save_audio)\n",
        "\n",
        "\n",
        "if not use_seed:\n",
        "  print(\"Generated from seed:\", y)\n",
        "else:\n",
        "  print(\"Generated from seed:\", seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPERIMENTAL: Timbre Transfer\n",
        "### Note: You need to Restart Runtime (Runtime > Restart Runtime) per each Timbre Transfer"
      ],
      "metadata": {
        "id": "uTuKspchQjY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import wav\n",
        "\n",
        "input_audio = \"your_audio_to_timbre_transfer.wav\" #@param {type:\"string\"}\n",
        "\n",
        "audio_in = single_audio_array(input_audio)\n",
        "aspec = tospec(audio_in)\n",
        "out_data = splitcut(aspec)"
      ],
      "metadata": {
        "id": "Z4LwV0anOG6S",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "rp7lQDQFQuct"
      },
      "source": [
        "#@title Build VAE Neural Network\n",
        "\n",
        "#VAE\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, Activation, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class VAE:\n",
        "  \"\"\"\n",
        "  VAE represents a Deep Convolutional autoencoder architecture\n",
        "  with mirrored encoder and decoder components.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_shape, #shape of the input data\n",
        "               conv_filters, #convolutional network filters\n",
        "               conv_kernels, #convNet kernel size\n",
        "               conv_strides, #convNet strides\n",
        "               latent_space_dim):\n",
        "    self.input_shape = input_shape # [28, 28, 1], in this case is 28 x 28 pixels on 1 channel for greyscale\n",
        "    self.conv_filters = conv_filters # is a list for each layer, i.e. [2, 4, 8]\n",
        "    self.conv_kernels = conv_kernels # list of kernels per layer, [1,2,3]\n",
        "    self.conv_strides = conv_strides # stride for each filter [1, 2, 2], note: 2 means you are downsampling the data in half\n",
        "    self.latent_space_dim = latent_space_dim # how many neurons on bottleneck\n",
        "    self.reconstruction_loss_weight = 1000000\n",
        "\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.model = None\n",
        "\n",
        "    self._num_conv_layers = len(conv_filters)\n",
        "    self._shape_before_bottleneck = None\n",
        "    self._model_input = None\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def summary(self):\n",
        "    self.encoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.decoder.summary()\n",
        "    print(\"\\n\")\n",
        "    self.model.summary()\n",
        "\n",
        "  def _build(self):\n",
        "    self._build_encoder()\n",
        "    self._build_decoder()\n",
        "    self._build_autoencoder()\n",
        "\n",
        "  def compile(self, learning_rate=0.0001):\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    self.model.compile(optimizer=optimizer, loss=self._calculate_combined_loss,\n",
        "                       metrics=[self._calculate_reconstruction_loss,\n",
        "                                self._calculate_kl_loss])\n",
        "\n",
        "  def train(self, x_train, batch_size, num_epochs):\n",
        "   # checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
        "   #                              save_best_only=True, mode='auto', period=1)\n",
        "    self.model.fit(x_train,\n",
        "                   x_train,\n",
        "                   batch_size=batch_size,\n",
        "                   epochs=num_epochs,\n",
        "                   shuffle=True)\n",
        "                   #callbacks=[checkpoint])\n",
        "\n",
        "  def save(self, save_folder=\".\"):\n",
        "    self._create_folder_if_it_doesnt_exist(save_folder)\n",
        "    self._save_parameters(save_folder)\n",
        "    self._save_weights(save_folder)\n",
        "\n",
        "  def load_weights(self, weights_path):\n",
        "    self.model.load_weights(weights_path)\n",
        "\n",
        "  def reconstruct(self, spec):\n",
        "      latent_representations = self.encoder.predict(spec)\n",
        "      reconstructed_spec = self.decoder.predict(latent_representations)\n",
        "      return reconstructed_spec, latent_representations\n",
        "  \n",
        "  def encode(self, spec):\n",
        "    latent_representation = self.encoder.predict(spec)\n",
        "    return latent_representation\n",
        "  \n",
        "  def sample_from_latent_space(self, z):\n",
        "      z_vector = self.decoder.predict(z)\n",
        "      return z_vector\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, save_folder=\".\"):\n",
        "      parameters_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(parameters_path, \"rb\") as f:\n",
        "          parameters = pickle.load(f)\n",
        "      autoencoder = VAE(*parameters)\n",
        "      weights_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      autoencoder.load_weights(weights_path)\n",
        "      return autoencoder\n",
        "\n",
        "  def _calculate_combined_loss(self, y_target, y_predicted):\n",
        "    reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
        "    kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
        "    combined_loss = self.reconstruction_loss_weight * reconstruction_loss + kl_loss\n",
        "    return combined_loss\n",
        "  \n",
        "  def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
        "    error = y_target - y_predicted\n",
        "    reconstruction_loss = K.mean(K.square(error), axis=[1, 2, 3])\n",
        "    return reconstruction_loss\n",
        "\n",
        "  def _calculate_kl_loss(self, y_target, y_predicted):\n",
        "    kl_loss = -0.5 * K.sum(1 + self.log_variance - K.square(self.mu) -\n",
        "                           K.exp(self.log_variance), axis =1)\n",
        "    return kl_loss\n",
        "\n",
        "  def _create_folder_if_it_doesnt_exist(self, folder):\n",
        "      if not os.path.exists(folder):\n",
        "          os.makedirs(folder)\n",
        "\n",
        "  def _save_parameters(self, save_folder):\n",
        "      parameters = [\n",
        "          self.input_shape,\n",
        "          self.conv_filters,\n",
        "          self.conv_kernels,\n",
        "          self.conv_strides,\n",
        "          self.latent_space_dim\n",
        "      ]\n",
        "      save_path = os.path.join(save_folder, \"parameters.pkl\")\n",
        "      with open(save_path, \"wb\") as f:\n",
        "          pickle.dump(parameters, f)\n",
        "\n",
        "  def _save_weights(self, save_folder):\n",
        "      save_path = os.path.join(save_folder, \"weights.h5\")\n",
        "      self.model.save_weights(save_path)\n",
        "\n",
        "#-----------AUTOENCODER----------#\n",
        "\n",
        "  def _build_autoencoder(self):\n",
        "    model_input = self._model_input\n",
        "    model_output = self.decoder(self.encoder(model_input))\n",
        "    self.model = Model(model_input, model_output, name=\"autoencoder\")\n",
        "\n",
        "#--------------DECODER------------#\n",
        "\n",
        "  def _build_decoder(self):\n",
        "    decoder_input = self._add_decoder_input()\n",
        "    dense_layer = self._add_dense_layer(decoder_input)\n",
        "    reshape_layer = self._add_reshape_layer(dense_layer)\n",
        "    conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
        "    decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
        "    self.decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
        "\n",
        "  def _add_decoder_input(self):\n",
        "    return Input(shape=self.latent_space_dim, name=\"decoder_input\")\n",
        "\n",
        "  def _add_dense_layer(self, decoder_input):\n",
        "    num_neurons = np.prod(self._shape_before_bottleneck) # [ 1, 2, 4] -> 8\n",
        "    dense_layer = Dense(num_neurons, name=\"decoder_dense\")(decoder_input)\n",
        "    return dense_layer\n",
        "\n",
        "  def _add_reshape_layer(self, dense_layer):\n",
        "    return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
        "\n",
        "  def _add_conv_transpose_layers(self, x):\n",
        "    \"\"\"Add conv transpose blocks.\"\"\"\n",
        "    # Loop through all the conv layers in reverse order and\n",
        "    # stop at the first layer\n",
        "    for layer_index in reversed(range(1, self._num_conv_layers)):\n",
        "      x = self._add_conv_transpose_layer(layer_index, x)\n",
        "    return x\n",
        "\n",
        "  def _add_conv_transpose_layer(self, layer_index, x):\n",
        "    layer_num = self._num_conv_layers - layer_index\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters=self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{layer_num}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    x = ReLU(name=f\"decoder_relu_{layer_num}\")(x)\n",
        "    x = BatchNormalization(name=f\"decoder_bn_{layer_num}\")(x)\n",
        "    return x\n",
        "\n",
        "  def _add_decoder_output(self, x):\n",
        "    conv_transpose_layer = Conv2DTranspose(\n",
        "        filters = 1,\n",
        "        kernel_size = self.conv_kernels[0],\n",
        "        strides = self.conv_strides[0],\n",
        "        padding = \"same\",\n",
        "        name=f\"decoder_conv_transpose_layer_{self._num_conv_layers}\"\n",
        "    )\n",
        "    x = conv_transpose_layer(x)\n",
        "    output_layer = Activation(\"sigmoid\", name=\"sigmoid_output_layer\")(x)\n",
        "    return output_layer\n",
        "\n",
        "#----------------ENCODER-----------------#\n",
        "\n",
        "  def _build_encoder(self):\n",
        "    encoder_input = self._add_encoder_input()\n",
        "    conv_layers = self._add_conv_layers(encoder_input)\n",
        "    bottleneck =  self._add_bottleneck(conv_layers)\n",
        "    self._model_input = encoder_input\n",
        "    self.encoder = Model(encoder_input, bottleneck, name=\"encoder\")\n",
        "\n",
        "  def _add_encoder_input(self):\n",
        "    return Input(shape=self.input_shape, name=\"encoder_input\")\n",
        "\n",
        "  def _add_conv_layers(self, encoder_input):\n",
        "    \"\"\"Creates all convolutional blocks in encoder\"\"\"\n",
        "    x = encoder_input\n",
        "    for layer_index in range(self._num_conv_layers):\n",
        "      x = self._add_conv_layer(layer_index, x)\n",
        "    return x\n",
        "  \n",
        "  def _add_conv_layer(self, layer_index, x):\n",
        "    \"\"\"Adds a convolutional block to a graph of layers, consisting\n",
        "    of Conv 2d + ReLu activation + batch normalization.\n",
        "    \"\"\"\n",
        "    layer_number = layer_index + 1\n",
        "    conv_layer = Conv2D(\n",
        "        filters= self.conv_filters[layer_index],\n",
        "        kernel_size = self.conv_kernels[layer_index],\n",
        "        strides = self.conv_strides[layer_index],\n",
        "        padding = \"same\",\n",
        "        name = f\"encoder_conv_layer_{layer_number}\"\n",
        "    )\n",
        "    x = conv_layer(x)\n",
        "    x = ReLU(name=f\"encoder_relu_{layer_number}\")(x)\n",
        "    x = BatchNormalization(name=f\"encoder_bn_{layer_number}\")(x)\n",
        "    return x\n",
        "\n",
        "#-------------Bottleneck (Latent Space)-------------#\n",
        "\n",
        "  def _add_bottleneck(self, x):\n",
        "    \"\"\"Flatten data and add bottleneck with Gaussian sampling (Dense layer)\"\"\"\n",
        "    self._shape_before_bottleneck = K.int_shape(x)[1:]\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.latent_space_dim,name=\"mu\")(x)\n",
        "    self.log_variance = Dense(self.latent_space_dim,\n",
        "                              name=\"log_variance\")(x)\n",
        "    \n",
        "    def sample_point_from_normal_distribution(args):\n",
        "      mu, log_variance = args\n",
        "      epsilon = K.random_normal(shape=K.shape(self.mu), mean=0., \n",
        "                                stddev=1.)\n",
        "      sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
        "\n",
        "      return sampled_point\n",
        "\n",
        "    x = Lambda(sample_point_from_normal_distribution, \n",
        "               name=\"encoder_output\")([self.mu, self.log_variance])\n",
        "    return x\n",
        "\n",
        "print(\"VAE successfully built\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Jf74jyfyQ2SF"
      },
      "source": [
        "#@title Load Checkpoint for Generating\n",
        "\n",
        "checkpoint_load_directory = \"/path/to/your/checkpoint\" #@param {type:\"string\"}\n",
        "\n",
        "#-------LOAD MODEL FOR GENERATING-------------#\n",
        "\n",
        "vae = VAE.load(checkpoint_load_directory)\n",
        "print(\"Loaded checkpoint\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Z2JISyMiQ6zt"
      },
      "source": [
        "#@title Import synthesis utility functions\n",
        "\n",
        "#-----TESTING FUNCTIONS ----------- #\n",
        "\n",
        "def select_spec(spec, labels, num_spec=10):\n",
        "    sample_spec_index = np.random.choice(range(len(spec)), num_spec)\n",
        "    sample_spec = spec[sample_spec_index]\n",
        "    sample_labels = labels[sample_spec_index]\n",
        "    return sample_spec, sample_labels\n",
        "\n",
        "\n",
        "def plot_reconstructed_spec(spec, reconstructed_spec):\n",
        "    fig = plt.figure(figsize=(15, 3))\n",
        "    num_spec = len(spec)\n",
        "    for i, (image, reconstructed_image) in enumerate(zip(spec, reconstructed_spec)):\n",
        "        image = image.squeeze()\n",
        "        ax = fig.add_subplot(2, num_spec, i + 1)\n",
        "        ax.axis(\"off\")\n",
        "        ax.imshow(image, cmap=\"gray_r\")\n",
        "        reconstructed_image = reconstructed_image.squeeze()\n",
        "        ax = fig.add_subplot(2, num_spec, i + num_spec + 1)\n",
        "        ax.axis(\"off\")\n",
        "        ax.imshow(reconstructed_image, cmap=\"gray_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_spec_encoded_in_latent_space(latent_representations, sample_labels):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(latent_representations[:, 0],\n",
        "                latent_representations[:, 1],\n",
        "                cmap=\"rainbow\",\n",
        "                c=sample_labels,\n",
        "                alpha=0.5,\n",
        "                s=2)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "#---------------NOISE GENERATOR FUNCTIONS ------------#\n",
        "\n",
        "def generate_random_z_vect(seed=1001,size_z=1,scale=1.0):\n",
        "    np.random.seed(seed)\n",
        "    x = np.random.uniform(low=(scale * -1.0), high=scale, size=(size_z,VECTOR_DIM))\n",
        "    return x\n",
        "\n",
        "def generate_z_vect_from_perlin_noise(seed=1001, size_z=1, scale=1.0):\n",
        "    np.random.seed(seed)\n",
        "    x = generate_perlin_noise_2d((size_z, VECTOR_DIM), (1,1))\n",
        "    x = x*scale\n",
        "    return x\n",
        "\n",
        "def generate_z_vect_from_fractal_noise(seed=1001, size_z=1, scale=1.0,):\n",
        "    np.random.seed(seed)\n",
        "    x = generate_fractal_noise_2d((size_z, VECTOR_DIM), (1,1),)\n",
        "    x = x*scale\n",
        "    return x\n",
        "\n",
        "\n",
        "#-------SPECTROGRAM AND SOUND SYNTHESIS UTILITY FUNCTIONS -------- #\n",
        "\n",
        "#Assembling generated Spectrogram chunks into final Spectrogram\n",
        "def specass(a,spec):\n",
        "  but=False\n",
        "  con = np.array([])\n",
        "  nim = a.shape[0]\n",
        "  for i in range(nim-1):\n",
        "    im = a[i]\n",
        "    im = np.squeeze(im)\n",
        "    if not but:\n",
        "      con=im\n",
        "      but=True\n",
        "    else:\n",
        "      con = np.concatenate((con,im), axis=1)\n",
        "  diff = spec.shape[1]-(nim*shape)\n",
        "  a = np.squeeze(a)\n",
        "  con = np.concatenate((con,a[-1,:,-diff:]), axis=1)\n",
        "  return np.squeeze(con)\n",
        "\n",
        "#Splitting input spectrogram into different chunks to feed to the generator\n",
        "def chopspec(spec):\n",
        "  dsa=[]\n",
        "  for i in range(spec.shape[1]//shape):\n",
        "    im = spec[:,i*shape:i*shape+shape]\n",
        "    im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
        "    dsa.append(im)\n",
        "  imlast = spec[:,-shape:]\n",
        "  imlast = np.reshape(imlast, (imlast.shape[0],imlast.shape[1],1))\n",
        "  dsa.append(imlast)\n",
        "  return np.array(dsa, dtype=np.float32)\n",
        "\n",
        "#Converting from source Spectrogram to target Spectrogram\n",
        "def towave_reconstruct(spec, spec1, name, path='../content/', show=False, save=False):\n",
        "  specarr = chopspec(spec)\n",
        "  specarr1 = chopspec(spec1)\n",
        "  print(specarr.shape)\n",
        "  a = specarr\n",
        "  print('Generating...')\n",
        "  ab = specarr1\n",
        "  print('Assembling and Converting...')\n",
        "  a = specass(a,spec)\n",
        "  ab = specass(ab,spec1)\n",
        "  awv = deprep(a)\n",
        "  abwv = deprep(ab)\n",
        "  if save:\n",
        "    print('Saving...')\n",
        "    pathfin = f'{path}/{name}'\n",
        "    sf.write(f'{pathfin}.wav', awv, sr)\n",
        "    print('Saved WAV!')\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  if show:\n",
        "    fig, axs = plt.subplots(ncols=2)\n",
        "    axs[0].imshow(np.flip(a, -2), cmap=None)\n",
        "    axs[0].axis('off')\n",
        "    axs[0].set_title('Reconstructed')\n",
        "    axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
        "    axs[1].axis('off')\n",
        "    axs[1].set_title('Input')\n",
        "    plt.show()\n",
        "  return abwv\n",
        "\n",
        "#Converting from Z vector generated spectrogram to waveform\n",
        "def towave_from_z(spec, name, path='../content/', show=False, save=False):\n",
        "  specarr = chopspec(spec)\n",
        "  print(specarr.shape)\n",
        "  a = specarr\n",
        "  print('Generating...')\n",
        "  print('Assembling and Converting...')\n",
        "  a = specass(a,spec)\n",
        "  awv = deprep(a)\n",
        "  if save:\n",
        "    print('Saving...')\n",
        "    pathfin = f'{path}/{name}'\n",
        "    sf.write(f'{pathfin}.wav', awv, sr)\n",
        "    print('Saved WAV!')\n",
        "  IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
        "  if show:\n",
        "    fig, axs = plt.subplots(ncols=1)\n",
        "    axs.imshow(np.flip(a, -2), cmap=None)\n",
        "    axs.axis('off')\n",
        "    axs.set_title('Decoder Synthesis')\n",
        "    plt.show()\n",
        "  return awv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform Timbre Transfer\n",
        "\n",
        "save_audio = True #@param {type:\"boolean\"}\n",
        "audio_name = \"timbre_transfered_audio\" #@param {type:\"string\"}\n",
        "audio_save_directory = \"/content/\" #@param {type:\"string\"}\n",
        "\n",
        "encoded_spec = vae.encode(out_data)\n",
        "print(np.shape(encoded_spec))\n",
        "\n",
        "z_sample = np.array(vae.sample_from_latent_space(encoded_spec))\n",
        "\n",
        "assembled_spec = testass(z_sample)\n",
        "towave_from_z(assembled_spec, name=audio_name,path=audio_save_directory,show=True, save=save_audio)"
      ],
      "metadata": {
        "id": "nt4lLm7Dxgfi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}